@article{JANOS,
author = {Bergman, David and Huang, Teng and Brooks, Philip and Lodi, Andrea and Raghunathan, Arvind U.},
title = {JANOS: An Integrated Predictive and Prescriptive Modeling Framework},
journal = {INFORMS Journal on Computing},
volume = {34},
number = {2},
pages = {807-816},
year = {2022},
doi = {10.1287/ijoc.2020.1023},

URL = {
        https://doi.org/10.1287/ijoc.2020.1023

},
eprint = {
        https://doi.org/10.1287/ijoc.2020.1023

}
}

@article{fischetti_jo_2018,
author = {Fischetti, Matteo and Jo, Jason},
year = {2018},
title = {Deep neural networks and mixed integer linear optimization},
journal = {Constraints},
volume = {23},
number = {2},
pages = {296-309},
URL = {https://doi.org/10.1007/s10601-018-9285-6},
doi = {10.1007/s10601-018-9285-6}
}

@article{Henao_Maravelias_2011,
author = {Henao, Carlos A. and Maravelias, Christos T.},
title = {Surrogate-based superstructure optimization framework},
journal = {AIChE Journal},
volume = {57},
number = {5},
pages = {1216-1232},
keywords = {process synthesis, process optimization, surrogate models},
doi = {https://doi.org/10.1002/aic.12341},
year = {2011}
}

@inproceedings{Lu_Pu_2017,
 author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Expressive Power of Neural Networks: A View from the Width},
 url = {https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{Schweidtmann_2022,
  doi = {10.48550/ARXIV.2207.12722},
  url = {https://arxiv.org/abs/2207.12722},
  author = {Schweidtmann, Artur M. and Bongartz, Dominik and Mitsos, Alexander},
  keywords = {Optimization and Control (math.OC), FOS: Mathematics, FOS: Mathematics, 90C26, 90C30, 90C90, 68T01, 60-04},
  title = {Optimization with Trained Machine Learning Models Embedded},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}


@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{ceccon2022omlt,
     title={OMLT: Optimization & Machine Learning Toolkit},
     author={Ceccon, F. and Jalving, J. and Haddad, J. and Thebelt, A. and Tsay, C. and Laird, C. D. and Misener, R.},
     year={2022},
     eprint={2202.02414},
     archivePrefix={arXiv},
     primaryClass={stat.ML}
}

  @misc{OptiCL,
    author = "Donato Maragno and Holly Wiberg",
    title = "OptiCL: Mixed-integer optimization with constraint learning",
    year = 2021,
    url = "https://github.com/hwiberg/OptiCL/"
  }

@misc{Maragano.et.al2021,
  doi = {10.48550/ARXIV.2111.04469},

  url = {https://arxiv.org/abs/2111.04469},

  author = {Maragno, Donato and Wiberg, Holly and Bertsimas, Dimitris and Birbil, S. Ilker and Hertog, Dick den and Fajemisin, Adejuyigbe},

  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Mixed-Integer Optimization with Constraint Learning},

  publisher = {arXiv},

  year = {2021},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{reluMIP.2021,
  title={reluMIP: Open Source Tool for MILP Optimization of ReLU Neural Networks},
  author={Lueg, Laurens and Grimstad, Bjarne and Mitsos, Alexander and Schweidtmann, Artur M.},
  year={2021},
  doi={https://doi.org/10.5281/zenodo.5601907},
  url = {https://github.com/ChemEngAI/ReLU_ANN_MILP},
  version = {1.0.0}
}

@article{GRIMSTAD2019106580,
title = {ReLU networks as surrogate models in mixed-integer linear programs},
journal = {Computers & Chemical Engineering},
volume = {131},
pages = {106580},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.106580},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419307203},
author = {Bjarne Grimstad and Henrik Andersson},
keywords = {Deep neural networks, ReLU networks, Mixed-Integer linear programming, Surrogate modeling, Regression},
}`


@article{Strong-mixed-integer-programming-formulations-for-trained-FULL,
author = {R. Anderson and J. Huchette and W. Ma and C. Tjandraatmadja and J.~P. Vielma},
title = {{Strong mixed-integer programming formulations for trained neural networks}},
journal = {Mathematical Programming},
volume ={183},
pages={3--39},
year ={2020}}

@InProceedings{The-Convex-Relaxation-Barrier-Revisited,
author = {C. Tjandraatmadja and R. Anderson and J. Huchette and W. Ma and K. Patel and J.~P. Vielma},
title = {{The Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification}},
pages={21675--21686},
editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
volume = {33},
series ={Advances in Neural Information Processing Systems},
booktitle ={Proceedings of the 34th Conference on Neural Information Processing Systems ({NeurIPS 2020})},
year ={2020}
}

@inproceedings{betweensteps,
author = {Kronqvist, Jan and Misener, Ruth and Tsay, Calvin},
title = {Between Steps: Intermediate Relaxations Between Big-M and Convex Hull Formulations},
year = {2021},
isbn = {978-3-030-78229-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78230-6_19},
doi = {10.1007/978-3-030-78230-6_19},
abstract = {This work develops a class of relaxations in between the big-M and convex hull formulations of disjunctions, drawing advantages from both. The proposed “P-split” formulations split convex additively separable constraints into P partitions and form the convex hull of the partitioned disjuncts. Parameter P represents the trade-off of model size vs. relaxation strength. We examine the novel formulations and prove that, under certain assumptions, the relaxations form a hierarchy starting from a big-M equivalent and converging to the convex hull. We computationally compare the proposed formulations to big-M and convex hull formulations on a test set including: K-means clustering, P_ball problems, and ReLU neural networks. The computational results show that the intermediate P-split formulations can form strong outer approximations of the convex hull with fewer variables and constraints than the extended convex hull formulations, giving significant computational advantages over both the big-M and convex hull.},
booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research: 18th International Conference, CPAIOR 2021, Vienna, Austria, July 5–8, 2021, Proceedings},
pages = {299–314},
numpages = {16},
keywords = {Disjunctive programming, Convex MINLP, Mixed-integer programming, Formulations, Relaxation comparison},
location = {Vienna, Austria}
}
